{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles of Data Reduction\n",
    "\n",
    "1. Sufficient Statistic\n",
    "2. Minimal Sufficient Statistic\n",
    "3. Ancillary Statistic\n",
    "4. Complete Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the \"best\" point estimator\n",
    "\n",
    "- We have seen that the MLE of $\\sigma^2$ in $N(\\mu,\\sigma^2)$ has a smaller MSE than the sample variance $S^2$.\n",
    "- Could we find an estimator of $\\sigma^2$ that has the smallest possible MSE?\n",
    "- Hard to answer in general, but if we restrict the space of estimators to e.g.\n",
    "    - all unbiased estimators, or\n",
    "    - all linear estimators, or\n",
    "    - all linear and unbiased estimators\n",
    "\n",
    "    we can sometimes find the estimator with the smallest possible MSE in that space.\n",
    "- UMVUE = Minimum Variance\n",
    "- BLUE = Best Linear Unbiased Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reduction\n",
    "- Want to use a sample $X_1,X_2,...,X_n$ to infer about an unknown parameter $\\theta$\n",
    "    - In practice: have data points $x_1,x_2,...,x_n$\n",
    "- A Statistic $T(X_1, X_2,...,X_n)$ is a method of summarizing the sample data\n",
    "    - In practice: $T(x_1,x_2,...,x_n)$\n",
    "- Is there a statistic $T(\\cdot)$ (or statistics $T_1(\\cdot),...,T_k(\\cdot)$) that gives the same amount of information about $\\theta$ as the sample $X_1,X_2,...,X_n$ does?\n",
    "    - Then we could store observed statistics only, instead of the whole dataset, i.e. get data reduction.\n",
    "- Main use for STAT 346: Add tools to find UMVUEs (Section 7.3.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistic as a partition of sample space\n",
    "- A statistic $T(X_1,X_2,...,X_n)$ can be thought of as a *partition* of the sample space $\\mathcal{X}$ of all possible outcomes for $\\mathbf{X}=(X_1,X_2,...,X_n)$\n",
    "- Let\n",
    "\n",
    "    $\\mathcal{T}=\\{t:t=T(x_1,x_2,...,x_n)\\text{ for some }\\mathbf{x}\\in\\mathcal{X}\\}$\n",
    "\n",
    "    - $\\mathcal{T}$ contains all possible outcomes of $T(X_1,X_2,...,X_n)$\n",
    "    - Often has lower dimension than $\\mathcal{X}$\n",
    "\n",
    "        - E.g. if $T(X_1,X_2,...,X_n)=\\bar{X}$ then $\\mathcal{X}=\\mathcal{R}^n$ and $\\mathcal{T}=\\mathcal{R}$\n",
    "- Partition of $\\mathcal{X}$:\n",
    "\n",
    "    $A_t=\\{\\mathbf{x}:T(\\mathbf{x})=t\\}\\quad t\\in\\mathcal{T}$\n",
    "\n",
    "    - Note: can have an uncoutable index space\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sufficient data reduction\n",
    "\n",
    "- A **sufficient statistic $T(X)$ for $\\theta$** (for a given distribution of $\\mathbf{X}$) is a statistic that contains (in some way) *all* the information about $\\theta$ in our sample $X_1,X_2,...,X_n$\n",
    "\n",
    "**Sufficiency Principle**\n",
    "\n",
    "If $T(X)$ is a sufficent statistic for $\\theta$, then any inference about $\\theta$ should depend on the sample $\\mathbf{X}$ only through the value $T(X)$.\n",
    "\n",
    "- I.e. if $\\mathbf{x}_1$ and $\\mathbf{x}_2$ are two sample outcomes such that $T(\\mathbf{x}_1)=T(\\mathbf{x_2})$ then the inference about $\\theta$ should be the same whether $\\mathbf{X}=\\mathbf{x}_1$ or $\\mathbf{X}=\\mathbf{x}_2$ was observed.\n",
    "\n",
    "- Can have more than one sufficient statistic for the same parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sufficient statistic\n",
    "\n",
    "**Def: Sufficient Statistic**\n",
    "\n",
    "A statistic $T(\\mathbf{X})$ is a sufficient statistic for $\\theta$ if\n",
    "\n",
    "$f(\\mathbf{x}|T(\\mathbf{X})=t)$ does not depend on $\\theta$\n",
    "\n",
    "i.e. the conditional distribution of the sample $\\mathbf{X}$ given the value of $T(\\mathbf{X})$ does not depend on $\\theta$.\n",
    "- Conditional probability = change of sample space\n",
    "    - $f(\\mathbf{x}| T(\\mathbf{X})=t)$ has support inside $A_t$\n",
    "- A sufficient statistic does not have to be one dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sense of the definition of a Sufficient Statistic\n",
    "Example $X_1,X_2,X_3$ i.i.d. $Bernoulli(\\theta)$ and $T(\\mathbf{X})=X_1+X_2+X_3$\n",
    "\n",
    "| $f(\\mathbf{X})$        | $x_1$ | $x_2$ | $x_3$ | $T(X)$ | $A_t$ |\n",
    "|:----------------------:|:-----:|:-----:|:-----:|:------:|:-----:|\n",
    "| $ (1-\\theta)^3 $       | 0     | 0     | 0     | 0      | $A_0$ | \n",
    "| $ \\theta(1-\\theta)^2 $ | 0     | 0     | 1     | 1      | $A_1$ |\n",
    "| $ \\theta(1-\\theta)^2 $ | 0     | 1     | 0     | 1      | $A_1$ | \n",
    "| $ \\theta(1-\\theta)^2 $ | 1     | 0     | 0     | 1      | $A_1$ |  \n",
    "| $ \\theta^2(1-\\theta) $ | 0     | 1     | 1     | 2      | $A_2$ |\n",
    "| $ \\theta^2(1-\\theta) $ | 1     | 0     | 1     | 2      | $A_2$ |\n",
    "| $ \\theta^2(1-\\theta) $ | 1     | 1     | 0     | 2      | $A_2$ |\n",
    "| $ \\theta^3 $           | 1     | 1     | 1     | 3      | $A_3$ | \n",
    "\n",
    "- The joint pdf:\n",
    "    \n",
    "    $f(\\mathbf{x}) = \\theta^{x_1}(1-\\theta)^{1-x_1} \\theta^{x_2}(1-\\theta)^{1-x_2} \\theta^{x_3}(1-\\theta)^{1-x_3}$\n",
    "\n",
    "    for $x_1,x_2,x_3\\in\\{0,1\\}\\times\\{0,1\\}\\times\\{0,1\\}$\n",
    "\n",
    "- What is the distribution of ($X_1,X_2,X_3$) given that $T(\\mathbf{X})=2$?\n",
    "\n",
    "    $f(\\mathbf{X}=\\mathbf{x}|T(\\mathbf{X})=2)=\\frac{P(\\mathbf{X}=\\mathbf{x},T(\\mathbf{X})=2)}{P(T(\\mathbf{X})=2)}$\n",
    "\n",
    "    Given that $T(\\mathbf{X})=2$, we know that the new sample space is $A_2=\\{(1,1,0),(1,0,1),(0,1,1)\\}$.\n",
    "\n",
    "    $f((0,1,1)|T(\\mathbf{X})=2)=\\frac{\\theta^2(1-\\theta)}{P(T(\\mathbf{X})=2)}=\\frac{1}{3}$\n",
    "\n",
    "    $f((1,0,1)|T(\\mathbf{X})=2)=\\frac{\\theta^2(1-\\theta)}{P(T(\\mathbf{X})=2)}=\\frac{1}{3}$\n",
    "\n",
    "    $f((1,1,0)|T(\\mathbf{X})=2)=\\frac{\\theta^2(1-\\theta)}{P(T(\\mathbf{X})=2)}=\\frac{1}{3}$\n",
    "\n",
    "    These three outcomes are equally likely, no matter what $\\theta$ is! $\\Rightarrow T(\\mathbf{X})$ is a sufficient statistic for $\\theta$. \n",
    "- in general, the outcomes in $A_t$ (the partition of the sample space we are conditioning on) will not always be equally likely for a sufficient statistic.\n",
    "    - Point is: we can find the conditional probability of all outcomes in $A_t$ without knowing the value of $\\theta$.\n",
    "        - For example: $f(\\mathbf{x}| T(\\mathbf{X})=t)$ does not depend on $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying a Sufficient statistic - Discrete case\n",
    "\n",
    "- Let's take a closer look at $f(\\mathbf{X}=\\mathbf{x}|T(\\mathbf{X})=t)$ for discrete random samples:\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    f(\\mathbf{X}=\\mathbf{x}|T(\\mathbf{X})=t)\n",
    "    &=\\frac{P(\\mathbf{X}=\\mathbf{x},T(\\mathbf{X}=t))}{P(T(\\mathbf{X})=t)}\\\\\n",
    "    &=\\frac{P(\\mathbf{X}=\\mathbf{x},\\mathbf{x}\\in A_t)}{P(T(\\mathbf{X})=t)}\\\\\n",
    "    &=\n",
    "    \\begin{cases}\n",
    "    \\frac{P(\\mathbf{X}=\\mathbf{x})}{P(T(\\mathbf{X})=t)}=\\frac{f(\\mathbf{x})}{f_T(t)}, &\\mathbf{x}\\in A_t \\\\\n",
    "    0, & \\mathbf{x}\\notin A_t    \n",
    "    \\end{cases}\n",
    "    \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying a Sufficient statistic\n",
    "\n",
    "**Theorem: Identifying a sufficient statistic**\n",
    "- Let $p(\\mathbf{x}|\\theta)$ be the joint pmf or pdf of $\\mathbf{X}=(X_1,...,X_n)$ and\n",
    "- Let $q(t|\\theta)$ be the pmf or pdf of a statistic $T(\\mathbf{X})$.\n",
    "\n",
    "If for every $\\mathbf{x}\\in\\mathcal{X}$ the ratio\n",
    "\n",
    "$\\frac{p(\\mathbf{x}|\\theta)}{q(T(\\mathbf{x})|\\theta)}$\n",
    "\n",
    "is a constant as a function of $\\theta$, then $T(\\mathbf{X})$ is a *sufficient statistic for $\\theta$*. \n",
    "- Need to find the pmf/pdf of $T(\\mathbf{X})$ (the sampling distribution) to use this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Possion\n",
    "- Let $X_1,X_2,...,X_n$ be a random sample from $Poisson(\\lambda)$. Show that $T(\\mathbf{X})=\\sum_{i=1}^n X_i$ is a sufficeint statistic for $\\lambda$.\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    P(\\mathbf{x}|\\lambda)\n",
    "    &=\\prod_{i=1}^n f(x_i|\\lambda)\\\\\n",
    "    &=\\prod_{i=1}^n \\frac{e^{-\\lambda}\\lambda^{x_i}}{x_i!}\\\\\n",
    "    &=e^{-n\\lambda}\\lambda^{\\sum_{i=1}^n x_i}\\prod_{i=1}^n\\frac{1}{x_i!}\n",
    "    \\end{aligned}$\n",
    "\n",
    "    Also need the pmf of $T(\\mathbf{X})=\\sum_{i=1}^n X_i$\n",
    "\n",
    "    Know that $T(\\mathbf{X})=t\\sim Poisson(n\\lambda)$, so $q(t|\\theta)=\\frac{e^{-n\\lambda}(n\\lambda)^{t}}{t!}$, where $t=\\sum_{i=1}^n X_i$.\n",
    "\n",
    "    $\\frac{p(\\mathbf{x}|\\theta)}{q(t|\\theta)}=\\frac{e^{-n\\lambda}\\lambda^{\\sum_{i=1}^n x_i}\\prod_{i=1}^n\\frac{1}{x_i!}}{\\frac{e^{-n\\lambda}(n\\lambda)^{t}}{t!}}=\\frac{t!}{n^t}\\prod_{i=1}^n\\frac{1}{x_i!}$ is a constant as a function of $\\theta$.\n",
    "\n",
    "    Therefore, $T(\\mathbf{X})=\\sum_{i=1}^n X_i$ is a sufficient statistic for $\\lambda$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: $Gamma^1$\n",
    "\n",
    "- Let $X_1,X_2,...,X_n$ be a random sample from $Gamma(1,\\theta)=Exp(\\theta)$. Show that $T(\\mathbf{X})=\\sum_{i=1}^n X_i$ is a sufficient statistic for $\\theta$.\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    P(\\mathbf{X}|\\theta)\n",
    "    &=f(\\mathbf{X}|\\theta)\\\\\n",
    "    &=\\prod_{i=1}^n\\frac{1}{\\theta}e^{-x_i/\\theta}\\\\\n",
    "    &=\\theta^{-n}e^{-\\frac{1}{\\theta}\\sum_{i=1}^n x_i}\n",
    "    \\end{aligned}$\n",
    "\n",
    "    $T(\\mathbf{X})=\\sum_{i=1}^n X_i = t\\sim Gamma(n,\\theta)$\n",
    "\n",
    "    $q(t|\\theta)=\\frac{1}{\\Gamma(n)\\theta^n}t^{n-1}e^{-t/\\theta}$, where $t = \\sum_{i=1}^n X_i$.\n",
    "\n",
    "    $\\frac{p(\\mathbf{x}|\\theta)}{q(t|\\theta)}=\\frac{\\theta^{-n}e^{-\\frac{1}{\\theta}\\sum_{i=1}^n x_i}}{\\frac{1}{\\Gamma(n)\\theta^n}t^{n-1}e^{-t/\\theta}}=\\frac{\\Gamma(n)}{t^{n-1}}$ is a constant as a function of $\\theta$.\n",
    "\n",
    "    Therefore, $t$ is a sufficient statistic for $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More about sufficient statistics\n",
    "\n",
    "- The original sample $\\mathbf{X}=(X_1,X_2,...,X_n)$ is it self a sufficient statistic\n",
    "    - $T(\\mathbf{X})=\\mathbf{X}\\sim P(\\mathbf{X}|\\theta)$\n",
    "    - $\\frac{p(\\mathbf{x}|\\theta)}{q(t|\\theta)}=1$\n",
    "- The vector of the $n$ order statistics\n",
    "\n",
    "    $T(\\mathbf{X})=(X_{(1)},X_{(2)},...,X_{(n)})$ which is a $n$ dimensional vector,\n",
    "\n",
    "    is always a sufficient statistic for a random sample:\n",
    "\n",
    "    - Order statistics joint pdf/pmf of $(X_{(1)},X_{(2)},...,X_{(n)})$ is\n",
    "\n",
    "        $n!f(x_1)f(x_2)...f(x_n)\\quad$ for $-\\infty<x_1<\\cdots<x_n<\\infty$ \n",
    "\n",
    "        so\n",
    "\n",
    "        $\\frac{p(\\mathbf{x}\\mid\\theta)}{q(T(\\mathbf{x})\\mid\\theta)}=\\frac{f(x_{1})f(x_{2})\\cdots f(x_{n})}{n!f(x_{1})f(x_{2})\\cdots f(x_{n})}=\\frac{1}{n!}$\n",
    "    - For some distributions the order statistics is as far as we can go with data reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find a sufficient statistics\n",
    "- Finding $q(T(\\mathbf{x})|\\theta)$ can be difficult. There are ways around it!\n",
    "\n",
    "**Factorization Theorem**:\n",
    "\n",
    "Let $f(\\mathbf{X}|\\theta)$ be the joint pmf or pdf of $\\mathbf{X}=(X_1,...,X_n)$. A statistic $T(\\mathbf{X})$ is a sufficient statistic for $\\theta$ if and only if $f(\\mathbf{x}|\\theta)$ can be written as\n",
    "\n",
    "$f(\\mathbf{x}|\\theta)=g(T(\\mathbf{x})|\\theta)h(\\mathbf{x})$\n",
    "\n",
    "- $g(T(\\mathbf{x})|\\theta)$ is function of $\\theta$ and $\\mathbf{x}$ only through $T(\\mathbf{x})$, i.e. could evaluate $g(T(\\mathbf{x})|\\theta)$ only knowing $T(\\mathbf{X})$\n",
    "- $h(\\mathbf{x})$ is function of $\\mathbf{x}$ only, no $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Poisson distribution\n",
    "\n",
    "$\\begin{aligned}\n",
    "    f(\\mathbf{x}|\\lambda)\n",
    "    &=\\prod_{i=1}^n f(x_i|\\lambda)\\\\\n",
    "    &=\\prod_{i=1}^n \\frac{e^{-\\lambda}\\lambda^{x_i}}{x_i!}\\\\\n",
    "    &=e^{-n\\lambda}\\lambda^{\\sum_{i=1}^n x_i}\\prod_{i=1}^n\\frac{1}{x_i!}\n",
    "\\end{aligned}$\n",
    "- $h(\\mathbf{x})=\\prod_{i=1}^n\\frac{1}{x_i!}$\n",
    "- $g(T(\\mathbf{x})=t|\\theta)=e^{-n\\lambda}\\lambda^{t}$, where $t=\\sum_{i=1}^n x_i$\n",
    "\n",
    "Therefore, $T(\\mathbf{x})=t=\\sum_{i=1}^n x_i$ is sufficient statistics for $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Normal distribution \n",
    "\n",
    "Let $X_1,X_2...,X_n$ be a random sample from $N(\\mu,\\sigma^2)$. Use the factorization theorem to show that the sample mean and sample variance are a sufficient statistic for $(\\mu,\\sigma^2)$ (both unknown)\n",
    "\n",
    "$\\begin{aligned}\n",
    "    f(\\mathbf{x}|\\mu,\\sigma^2)\n",
    "    &=\\prod_{i=1}^n f(x_i|\\mu,\\sigma^2)\\\\\n",
    "    &=\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(x_i-\\mu)^2}{2\\sigma^2})\\\\\n",
    "    &=(2\\pi)^{-n/2}\\sigma^{-n}exp(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2)\\\\\n",
    "    &=(2\\pi)^{-n/2}\\sigma^{-n}exp(-\\frac{1}{2\\sigma^2}((n-1)S^2+n(\\bar{x}-\\mu)^2))\\\\\n",
    "\\end{aligned}$\n",
    "\n",
    "- $h(\\mathbf{x})=(2\\pi)^{-n/2}$\n",
    "- $g(\\bar{X},S^2|\\mu,\\sigma^2)=\\sigma^{-n}exp(-\\frac{1}{2\\sigma^2}((n-1)S^2+n(\\bar{x}-\\mu)^2))$\n",
    "\n",
    "Therefore, $\\bar{X},S^2$ are sufficient statistics for $\\mu,\\sigma^2$, by Factorization Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Uniform distribution\n",
    "\n",
    "Let $X_1,X_2...,X_n$ be a random sample from $Uniform(0,\\theta)$. Use the factorization theorem to show that the $n$-the order statistic $X_{(n)}$ is a sufficient statistic for $\\theta$.\n",
    "\n",
    "$f(x_i|\\theta)=\\frac{1}{\\theta}I_{(0,\\theta)}(x_i)$ \n",
    "\n",
    "$\\begin{aligned}\n",
    "f(\\mathbf{x}|\\theta)\n",
    "&=\\prod_{i=1}^n\\frac{1}{\\theta}I_{(0,\\theta)}(x_i)\\\\\n",
    "&=\\theta^{-n}I_{(-\\infty,\\theta)}(x_{(n)})I_{(0,\\infty)}(x_{(1)})\n",
    "\\end{aligned}$\n",
    "\n",
    "- $h(\\mathbf{x})=I_{(0,\\infty)}(x_{(1)})$  \n",
    "- $g(x_{(n)}|\\theta)=\\theta^{-n}I_{(-\\infty,\\theta)}(x_{(n)})$\n",
    "\n",
    "Therefore, $x_{(n)}$ is a sufficient statistic for $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sufficient statistics for exponential families\n",
    "- Let $X_1,X_2,...,X_n$ be a random sample from pdf or pmf of the form\n",
    "\n",
    "    $f(x\\mid\\mathbf{\\theta})=h(x)c(\\mathbf{\\theta})\\exp\\left(\\sum_{j=1}^{k}w_{j}(\\mathbf{\\theta})t_{j}(x)\\right)$\n",
    "- The statistic\n",
    "\n",
    "    $T(\\mathbf{X})=\\left(\\sum_{i=1}^n t_1(X_i),\\sum_{i=1}^n t_2(X_i),\\ldots,\\sum_{i=1}^n t_k(X_i)\\right)$\n",
    "\n",
    "    is a sufficient statistic for $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "\n",
    "For each $i, i=1,2,...,n.$\n",
    "\n",
    "$f(x_i|\\theta)=h(x_i)c(\\mathbf{\\theta})exp\\big(t_1(x_i)w_1(\\mathbf{\\theta})+\\cdots+t_k(x_i)w_k(\\mathbf{\\theta})\\big)$\n",
    "\n",
    "Joint pdf or pmf:\n",
    "\n",
    "$\\begin{aligned}\n",
    "f(\\mathbf{x}|\\mathbf{\\theta})\n",
    "&=\\prod_{i=1}^n h(x_i)c(\\mathbf{\\theta})exp\\big(t_1(x_i)w_1(\\mathbf{\\theta})+\\cdots+t_k(x_i)w_k(\\mathbf{\\theta})\\big)\\\\\n",
    "&=\\prod_{i=1}^n h(x_i) \\cdot c(\\mathbf{\\theta})^n \\cdot exp\\big(w_1(\\mathbf{\\theta})\\sum_{i=1}^nt_1(x_i)+\\cdots+w_k(\\mathbf{\\theta})\\sum_{i=1}^nt_k(x_i)\\big)\\\\\n",
    "\\end{aligned}$\n",
    "- $h(\\mathbf{x})=\\prod_{i=1}^n h(x_i)$\n",
    "- $g\\left(\\sum_{i=1}^n t_1(X_i),\\sum_{i=1}^n t_2(X_i),\\ldots,\\sum_{i=1}^n t_k(X_i)|\\mathbf{\\theta}\\right)=c(\\mathbf{\\theta})^n \\cdot exp\\big(w_1(\\mathbf{\\theta})\\sum_{i=1}^nt_1(x_i)+\\cdots+w_k(\\mathbf{\\theta})\\sum_{i=1}^nt_k(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many sufficient statistics\n",
    "\n",
    "- For many distributions there are many different sufficient statistics\n",
    "- The whole sample $T(\\mathbf{X})=(X_{1},X_{2}...,X_{n})$ is a sufficient statistic\n",
    "- The set of all order statistics $T(\\mathbf{X})=(X_{(1)},X_{(2)}...,X_{(n)})$ is a sufficient statistic\n",
    "- Any one-to-one function of a sufficient statistic is a sufficient statistic\n",
    "    - Say, $T$ is sufficient statistic, let $c(T)=W\\Rightarrow T=C^{-1}(W)$ be one to one function\n",
    "\n",
    "        $f(\\mathbf{X}|\\mathbf{\\theta})=h(\\mathbf{X})g(c^{-1}(W)|\\mathbf{\\theta})$\n",
    "        - $g(c^{-1}(W)|\\mathbf{\\theta})$ is a function of $W$ conditioning on $\\mathbf{\\theta})$\n",
    "\n",
    "Can we find a sufficient statistics that gives us most \"data reduction\" possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Sufficient Statistic\n",
    "\n",
    "**Def: Minimal Sufficient Statistic**\n",
    "\n",
    "A sufficient statistic $T(\\mathbf{X})$ is called a *minimal sufficient statistic* if for any other sufficient statistic $T^*(\\mathbf{X})$, $T(\\mathbf{X})$  is a function of $T^*(\\mathbf{X})$.\n",
    "- E.g. $T(\\mathbf{X})=(X_{(1)},X_{(2)}...,X_{(n)})$ and $T=\\sum_{i=1}^n X_i$, then i.e. $T=\\sum_{i=1}^n X_{(i)}$ is a function of $T^*$.\n",
    "- Obtains the *coarsest* partition of the sample space as possible, without loosing any information about the parameter $\\theta$\n",
    "- Can still have many different minimal statistics\n",
    "    - One-to-one function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a Minimal Sufficient Statistic\n",
    "\n",
    "**Theorem:**\n",
    "\n",
    "Let $f(\\mathbf{x}|\\theta)$ be the pmf or pdf of a sample $\\mathbf{X}$. Suppose there exists a function $T(\\mathbf{X})$ such that:\n",
    "- For every two outcomes $\\mathbf{x}$ and $\\mathbf{y}$ (any $\\mathbf{x},\\mathbf{y}\\in\\mathcal{X}$) the ratio\n",
    "\n",
    "    $\\frac{f(\\mathbf{x}|\\theta)}{f(\\mathbf{y}|\\theta)}$\n",
    "\n",
    "    is a constant as a function of $\\theta$ *if and only if* $T(\\mathbf{x})=T(\\mathbf{y})$, then $T(X)$ is a minimal sufficient statistic.\n",
    "\n",
    "    That's to say:\n",
    "    1. If $T(\\mathbf{x})=T(\\mathbf{y})$, then $\\frac{f(\\mathbf{x}|\\theta)}{f(\\mathbf{y}|\\theta)}$ is a constant as a function of $\\theta$, thus $T(X)$ is a minimal sufficient statistic.\n",
    "    2. If $\\frac{f(\\mathbf{x}|\\theta)}{f(\\mathbf{y}|\\theta)}$ is a constant as a function of $\\theta$, then $T(\\mathbf{x})=T(\\mathbf{y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "$X_1,X_2,...,X_n$ are i.i.d $Poisson(\\lambda)$\n",
    "\n",
    "$f(\\mathbf{x}|\\lambda)=\\prod_{i=1}^{n}\\frac{e^{-\\lambda}\\lambda^{x_i}}{x_i!}=e^{-n\\lambda}\\lambda^{n\\bar{x}}\\prod_{i=1}^{n}\\frac{1}{x_i!}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\frac{f(\\mathbf{x}|\\lambda)}{f(\\mathbf{y}|\\lambda)}\n",
    "&=\\frac{e^{-n\\lambda}\\lambda^{n\\bar{x}}\\prod_{i=1}^{n}\\frac{1}{x_i!}}{e^{-n\\lambda}\\lambda^{n\\bar{y}}\\prod_{i=1}^{n}\\frac{1}{y_i!}}\\\\\n",
    "&=\\lambda^{n(\\bar{x}-\\bar{y})}\\prod_{i=1}^{n}\\frac{y_i!}{x_i!}\n",
    "\\end{aligned}$\n",
    "\n",
    "1. If $\\bar{x}=\\bar{y}$, then $\\frac{f(\\mathbf{x}|\\lambda)}{f(\\mathbf{y}|\\lambda)}=\\prod_{i=1}^{n}\\frac{y_i!}{x_i!}$ is a constant as a function of $\\lambda$.\n",
    "2. If $\\bar{x}\\neq\\bar{y}$, then $\\frac{f(\\mathbf{x}|\\lambda)}{f(\\mathbf{y}|\\lambda)}$ does depend on $\\lambda$.\n",
    "\n",
    "Therefore, $T(\\mathbf{x})=\\bar{x}$ is a minimal sufficient statistic for $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Normal distribution\n",
    "\n",
    "- Let $X_1,X_2,...,X_n$ be a random sample from $N(\\mu,\\sigma^2)$, both $\\mu$ and $\\sigma^2$ are unknown. Find a minimal sufficient statistic for $(\\mu,\\sigma^2)$. \n",
    "\n",
    "    $f(\\mathbf{x}|\\mu,\\sigma^2)=(2\\pi\\sigma^2)^{-n/2}exp(-\\frac{1}{2\\sigma^2}((n-1)s^2+n(\\bar{x}-\\mu)^2))$\n",
    "\n",
    "    $\\frac{f(\\mathbf{x}|\\mu,\\sigma^2)}{f(\\mathbf{y}|\\mu,\\sigma^2)}=\\frac{exp(-\\frac{1}{2\\sigma^2}((n-1)s_x^2+n(\\bar{x}-\\mu)^2))}{exp(-\\frac{1}{2\\sigma^2}((n-1)s_y^2+n(\\bar{y}-\\mu)^2))}=exp(-\\frac{1}{2\\sigma^2}((n-1)(s_x^2-s_y^2)+n(\\bar{x}+\\bar{y}-2\\mu)(\\bar{x}-\\bar{y})))\\cdots(*)$\n",
    "\n",
    "    where $s_x^2=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2$ and $s_y^2=\\frac{1}{n-1}\\sum_{i=1}^n(y_i-\\bar{y})^2$\n",
    "\n",
    "    $(*)$ is a constant as a function of $\\mu$ and $\\sigma^2$, if and only if $s_x^2=s_y^2$ and $\\bar{x}=\\bar{y}$.\n",
    "\n",
    "    Therefore, $S^2$ and $\\bar{X}$ are minimal sufficient statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary statistic\n",
    "\n",
    "**Def: Ancillary statistic**\n",
    "\n",
    "A statistic $S(\\mathbf{X})$ is called an *ancillary statistic* if its distribution does not depend on the parameter $\\theta$. E.g. no information about $\\theta$ in $S(\\mathbf{X})$.\n",
    "- Kind of an opposite to a sufficient statistic\n",
    "- There are a few examples of ancillary statistics actually providing information about a parameter when combined with a sufficient statistic\n",
    "    - See examples in the textbook\n",
    "- But often sufficient and ancillary statistics are *statistically independent*\n",
    "    - At least if the sufficient statistic is *complete*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Ancillary Statistics\n",
    "- Say $X_1,X_2,...,X_n$ is a random sample from $N(\\mu,\\sigma^2)$ where $\\sigma^2$ is known.\n",
    "- We have seen that\n",
    "\n",
    "    $\\frac{(n-1)S^2}{\\sigma^2}\\sim\\chi_{n-1}^2$\n",
    "\n",
    "    The distribution of $S^2$ does not depend on $\\mu$, so $S^2$ is an ancillary statistic for $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete statistic\n",
    "\n",
    "**Def: Complete Statistic**\n",
    "\n",
    "Let $f(t|\\theta)$ be a family of pdfs or pmfs for a statistic $T(\\mathbf{X})$. The family is called complete if the following holds\n",
    "- If $E(g(T))=0$ for all $\\theta$ then $P(g(T)=0)=1$ for all $\\theta$.\n",
    "    - $g(T)$ is a unbiased estimator of 0.\n",
    "\n",
    "Also, the statistic $T(\\mathbf{X})$ is called a complete statistic\n",
    "- Property of the *family* of distribution $f(t|\\theta)$ belongs to\n",
    "- Can be hard to verify sometimes\n",
    "    - See examples 6.2.22 and 6.2.23 in the textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completeness of exponential families\n",
    "\n",
    "**Theorem 6.2.25**\n",
    "\n",
    "Let $X_1,X_2,...,X_n$ be a random sample from pdf or pmf of the form\n",
    "\n",
    "$f(x\\mid\\mathbf{\\theta})=h(x)c(\\mathbf{\\theta})\\exp\\left(\\sum_{j=1}^kw_j(\\mathbf{\\theta})t_j(x)\\right)$\n",
    "\n",
    "The statistic\n",
    "\n",
    "$T(\\mathbf{X})=\\left(\\sum_{i=1}^nt_1(X_i),\\sum_{i=1}^nt_2(X_i),\\ldots,\\sum_{i=1}^nt_k(X_i)\\right)$\n",
    "\n",
    "is complete if $\\{(w_1(\\mathbf{\\theta}),\\ldots,w_k(\\mathbf{\\theta})):\\mathbf{\\theta}\\in\\Theta\\}$ contains an open set in $\\mathbb{R}^k$\n",
    "\n",
    "- The condition means: the theorem does not hold for curved exponential families like $N(\\theta,\\theta^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "- Let $X_1,X_2,...,X_n$ be a random sample from $Poisson(\\lambda)$. Use theorem 6.2.25 to find a complete statistic.\n",
    "\n",
    "    $f(x|\\lambda)=\\frac{e^{-\\lambda}\\lambda^x}{x!}=\\frac{1}{x!}\\cdot e^{-\\lambda}\\cdot e^{x\\ln\\lambda}$\n",
    "\n",
    "    - $h(x)=\\frac{1}{x!}$\n",
    "    - $c(\\lambda)=e^{-\\lambda}$\n",
    "    - $w(\\lambda)=\\ln\\lambda$\n",
    "    - $t(x)=x$\n",
    "\n",
    "    Thus, it is a exponential family.\n",
    "\n",
    "    Therefore, $\\sum_{i=1}^n t(x_i)=\\sum_{i=1}^n x_i$ is a complete statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theorem regarding complete statistics\n",
    "\n",
    "**Basu's Theorem**\n",
    "\n",
    "If $T(\\mathbf{X})$ is a complete and minimal sufficient statistic, then $T(\\mathbf{X})$ is independent of every ancillary statistic.\n",
    "\n",
    "- E.g. $\\bar{X}$ is a compete and minimal sufficient statistic for $\\mu$, given a random sample from $N(\\mu,\\sigma^2)$.   \n",
    "    - Since $S^2$ is ancillary for $\\mu$, by Basu's Theorem, $\\bar{X}$ and $S^2$ are independent.\n",
    "\n",
    "**Theorem 6.2.28**\n",
    "\n",
    "If a minimal sufficient statistic exists, then any complete statistic is also a minimal sufficient statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Let $X_1,X_2,...,X_n$ be a random sample from $f(X|\\theta)$, and $T=T(\\mathbf{X})$. \n",
    "- How to show that $T$ is a sufficient statistic? 3 ways:\n",
    "    1. Definition: $f(\\mathbf{X}|T(\\mathbf{X})=t)$ is a constant as a function of $\\theta$.\n",
    "    2. $\\frac{f(\\mathbf{x}|\\theta)}{q(t|\\theta)}$ is a constant as a function of $\\theta$, then $T(\\mathbf{X})$ is a *sufficient statistic for $\\theta$*. \n",
    "        - where $q(t|\\theta)$ be the pmf or pdf of a statistic $T(\\mathbf{X})$.\n",
    "    3. Easiest: Factorization Theorem\n",
    "        - $f(\\mathbf{x}|\\theta)=g(T(\\mathbf{x})|\\theta)h(\\mathbf{x})$\n",
    "    4. Exponential families:\n",
    "        - $T(\\mathbf{X})=\\left(\\sum_{i=1}^n t_1(X_i),\\sum_{i=1}^n t_2(X_i),\\ldots,\\sum_{i=1}^n t_k(X_i)\\right)$\n",
    "- How to show that $T$ is a minimal sufficient statistic?\n",
    "    1. If $T(\\mathbf{x})=T(\\mathbf{y})$, then $\\frac{f(\\mathbf{x}|\\theta)}{f(\\mathbf{y}|\\theta)}$ is a constant as a function of $\\theta$, thus $T(X)$ is a minimal sufficient statistic.\n",
    "- How to show that $T$ is a complete statistic?\n",
    "    1. Exponential family:\n",
    "        - $T(\\mathbf{X})=\\left(\\sum_{i=1}^nt_1(X_i),\\sum_{i=1}^nt_2(X_i),\\ldots,\\sum_{i=1}^nt_k(X_i)\\right)$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
